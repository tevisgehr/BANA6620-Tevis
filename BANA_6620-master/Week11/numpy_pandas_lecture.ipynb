{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy \n",
    "\n",
    "This set of notes will provide as a reasonable toolkit to understanding the `Numpy` library.  As is frequently the case, I will begin by pointing you to the documentation: `Numpy` offers an amazing amount of functionality, as illustrated by their [documentation](http://www.numpy.org/). One of the most popular parts of this library is the introduction to the `numpy array`, which will be the center point for the notes that follow. \n",
    "\n",
    "## Numpy Arrays\n",
    "\n",
    "From a high level, a `numpy array` is like a list; a list that actually underlies all that we do in Pandas.  However, these arrays are much faster than lists. The two main reasons these arrays are faster: \n",
    "\n",
    "1. They are stored as one contiguous block of memory.  For lists, the allocation actually occurs across multiple locations. \n",
    "2. Each item in a numpy array is of the same data type.  Lists might hold strings, floats, ints, additional lists, etc.  \n",
    "\n",
    "Let's do a short example to see how these arrays perform compared to our traditional lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # Standard alias when importing numpy - follow this convention!\n",
    "numpy_array = np.arange(0, 1000000)\n",
    "python_list = range(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np.sum(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit sum(python_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit sum(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy stores data in contiguous blocks of memory that are able to take advantage of **vectorization**, which is the ability of a CPU to perform one operation on mulitiple pieces of data at once. Notice, using `np.sum()` - numpy's implementation of sum allowed us to speed up our calculations. However, when we just used the built-in python `sum()` on the numpy array, the calculation was actually slower. This illustrates the need to use vectorized operations with objects that have the ability to be vectorized. \n",
    "\n",
    "If you are interested in more than what is covered in this notebook, see the [documentation](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.ndarray.html#numpy.ndarray).\n",
    "\n",
    "Let's start with the creation of the `numpy array`. We use the `np.array()` constructor, and we can pass either a list or tuple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst_array = np.array([1, 2, 3, 4, 5])\n",
    "tuple_array = np.array((1, 2, 3, 4, 5), np.int32) \n",
    "my_array = np.array([[[1,2],[3,4],[5,6]], [[1,2],[3,4],[5,6]]])\n",
    "my_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the attributes we have on dataframes, we also have on our numpy arrays.  Let's take a look at some of the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print my_lst_ndarray.shape\n",
    "print my_tuple_ndarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print my_lst_ndarray.dtype\n",
    "print my_tuple_ndarray.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_lst_ndarray2 = np.array([\"1\", 2, 3, \"10\", 5])\n",
    "print my_lst_ndarray2.dtype\n",
    "my_lst_ndarray2[1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_lst_ndarray3 = np.array([1, 2, 3, \"10\", 5], np.int32) \n",
    "print my_lst_ndarray3.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some nice ways to build arrays quickly.  Let's look at a few popular means of creating these arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zeros_array = np.zeros((3,5)) \n",
    "ones_array = np.ones((10,20))  \n",
    "identity_array = np.identity(50)  \n",
    "random_array = np.random.rand(2, 2) \n",
    "range_array = np.arange(0, 20, 0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with a `numpy` array, we have all of the basic mathematics operators available to us - `+`, `-`, `*`, `/`, `**`, and `%`. Frequently, we'll be working with two arrays that are the same size, in which case these operators will just be performed **element-wise**. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_arr = np.array([1, 2, 3, 4])\n",
    "second_arr = np.array([5, 6, 7, 8])\n",
    "first_arr + second_arr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_arr = np.array([1, 2]) \n",
    "second_arr = np.array([[5, 6], [7, 8]]) \n",
    "first_arr * second_arr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that our numerical operations can also work when we want to perform an operation between a `numpy array` and a single value. For example, let's say that we want to subtract `4` from `first_arr` above, or multiply it by `5`, or find the remainder when everything is divided by `3`. We can do that via the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_arr = np.array([[1, 2], [3, 4]]) # This is now a two-dimensional array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_arr - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_arr * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_arr % 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept that allows this to happen is referred to as [broadcasting](http://docs.scipy.org/doc/numpy-1.10.1/user/basics.broadcasting.html). It is a concept that will be particularly useful when working with and interacting with `numpy` arrays. Basically, it takes that single number on the right (the `4`, `5`, or `3` above), and **broadcasts** it's shape to match that of `first_arr` (`2 x 2`). After doing so, it then performs the operation element-wise like we saw before. \n",
    "\n",
    "It turns out that things can get a little more intricate than this. If we wanted, we could perform mathematical operations like the above at a column level, or row level. For example, we could subtract off `4` from the first column and `5` from the second column, or `4` from the first row and `5` from the second row. We would do that via the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_arr = np.array([[1, 2, 4, 5], [3, 4, 8, 5]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_arr - [[4],[3]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_arr * [[4], [5]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Indexing \n",
    "\n",
    "We can index into arrays similar to the way we index into a pandas dataframe, but we don't have the `.loc[]`, `.iloc[]`, or `.ix[]` methods like we do on a DataFrame. Our arrays are like a multidimensional list. Let's look at how we can index into our arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_arr = np.arange(0, 20, 1).reshape(4, 5)\n",
    "print range_arr[:,0]\n",
    "print range_arr[:, 2] \n",
    "print range_arr[0:2]\n",
    "print range_arr[0:2, 1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print range_arr.sum(axis=0) \n",
    "print range_arr.sum(axis=1) \n",
    "print range_arr.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform additional aggregate functions on our arrays. We could also do this along the columns, rows, or for the array as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print range_arr.mean(axis=0)\n",
    "print range_arr.std(axis=0)\n",
    "print range_arr.max(axis=0)\n",
    "print range_arr.min(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we want to instead grab the **index** at which those min and max values occur (either along the rows or columns), then we can use the `argmin()` and `argmax()` methods available on our numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range_arr.argsort(axis=1)\n",
    "print range_arr.argmin(axis=0)\n",
    "print range_arr.argmin()\n",
    "print range_arr.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting pandas and numpy: Because Pandas DataFrames are built on numpy arrays the majority of the methods that we looked at for numpy arrays are available on Pandas series. It is worth noting, there might be slightly different naming conventions (`idxmax` on a column versus `argmax` on a numpy array).\n",
    "\n",
    "Many of these methods are available as functions on the `numpy` module itself, as well. Just like we can call the `argmax()` method on a numpy array, we can call `np.argmax()` and pass in a list or tuple. Before we move back to DataFrames, let's look at one last method that is available in `numpy`, `np.where()`. The `np.where()` functionality allows us to find what elements in a numpy array meet some condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_ndarray = np.array([2, 4, 6, 8, 24, 3, 8, 9, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.where(my_ndarray <= 2)[0][0] \n",
    "print np.where(my_ndarray == 8)[0][1] \n",
    "print np.where(my_ndarray > 6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Datasets with Pandas \n",
    "\n",
    "Pandas functions that allow us to combine two sets of data include the use of `pd.merge()`, `df.join()`, `df.merge()`, and `pd.concat()`. For the most part, these do largely the same things (although you'll notice the slight syntax difference with `merge()` and `concat()` being able to be called via the Pandas module and `merge` and `join()` being able to be called on a DataFrame instance). There are some cases where one of these might be better than another in terms of writing less code or performing some kind of data combination in an easier way. The major differences between these, though, largely depend on what they do by default when you try to combine different data. By default, `merge()` looks to join on common columns, `join()` on common indices, and `concat()` by just appending on a given axis.\n",
    "\n",
    "You can find more detail about the differences between all three of these in the [docs](http://pandas.pydata.org/pandas-docs/stable/merging.html). We'll look at some examples below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "red_wines_df = pd.read_csv('data/winequality-red.csv', delimiter=';')\n",
    "white_wines_df = pd.read_csv('data/winequality-white.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "red_wines_quality_df = red_wines_df.groupby('quality').mean()['fixed acidity'].reset_index()\n",
    "white_wines_quality_df = white_wines_df.groupby('quality').mean()['fixed acidity'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.merge(red_wines_quality_df, white_wines_quality_df, on=['quality'], suffixes=[' red', ' white'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tables\n",
    "\n",
    "From [wiki](https://en.wikipedia.org/wiki/Pivot_table): \"Among other functions, a pivot table can automatically sort, count total, or give the average of the data stored in one table or spreadsheet, displaying the results in a second table showing the summarized data. Pivot tables are also useful for quickly creating unweighted cross tabulations.\"\n",
    "\n",
    "As you might have guessed, we have functionality to create pivot tables available for our use in Pandas. The way that we do this is by calling the `pivot_table()` function that is available on the pandas module (which we've stored as `pd`). As the [docs](http://pandas.pydata.org/pandas-docs/stable/reshaping.html#pivot-tables-and-cross-tabulations) tell us, the `pivot_table()` expects a number of different arguments: \n",
    "\n",
    "1. `data`: A DataFrame object\n",
    "2. `values`: a column or a list of columns to aggregate\n",
    "3. `index`: a column, Grouper, array which has the same length as data, or list of them. Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values.\n",
    "4. `columns`: a column, Grouper, array which has the same length as data, or list of them. Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values.\n",
    "5. `aggfunc`: function to use for aggregation, defaulting to numpy.mean\n",
    "\n",
    "Notice that by default this uses the mean for the `aggfunc` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.cut(red_wines_df['fixed acidity'], bins=np.arange(4, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fixed_acidity_bins = np.arange(4, 17)\n",
    "fixed_acidity_series = pd.cut(red_wines_df['fixed acidity'], bins=fixed_acidity_bins, \n",
    "                              labels=fixed_acidity_bins[:-1])\n",
    "fixed_acidity_series.name = 'fa_bin'\n",
    "red_wines_df = pd.concat([red_wines_df, fixed_acidity_series], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(red_wines_df, values='residual sugar', index='quality', \n",
    "               columns='fa_bin', aggfunc=np.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
